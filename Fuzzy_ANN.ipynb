{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQeT9sF8BTn0y/VqOXlZ7x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Subhash-K45/python/blob/main/Fuzzy_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# Define the input data\n",
        "data = np.array([\n",
        "    ['Low', 'Low', 'Medium', 'Very High'],\n",
        "    ['Medium', 'Very Low', 'High', 'Low'],\n",
        "    ['High', 'Low', 'High', 'Medium'],\n",
        "    ['Very Low', 'Very Low', 'Low', 'Low']\n",
        "])\n",
        "\n",
        "# Separate features and target\n",
        "X = data[:, 1:]\n",
        "y = data[:, 0]\n",
        "\n",
        "# Initialize label encoders\n",
        "label_encoders = [LabelEncoder() for _ in range(X.shape[1])]\n",
        "\n",
        "# Fit label encoders and transform input data\n",
        "X_encoded = np.column_stack([le.fit_transform(X[:, i]) for i, le in enumerate(label_encoders)])\n",
        "\n",
        "# One-hot encode target variable\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "y_encoded = onehot_encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Build the ANN model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(8, input_shape=(X_encoded.shape[1],), activation='relu'),\n",
        "    tf.keras.layers.Dense(4, activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_encoded, y_encoded, epochs=100, batch_size=1, verbose=1)"
      ],
      "metadata": {
        "id": "1E0gJde5kihL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "for i in range(len(data)):\n",
        "  l=[]\n",
        "  print(i)\n",
        "  for j in range(len(data[i])):\n",
        "    new_data = np.array([\n",
        "    np.concatenate((data[i][:j], data[i][j+1:]))\n",
        "])\n",
        "\n",
        "    # Transform the new data using the fitted label encoders\n",
        "    X_new_encoded = np.column_stack([le.transform(new_data[:, i]) for i, le in enumerate(label_encoders)])\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(X_new_encoded)\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "    # Decode predicted classes\n",
        "    print(predicted_classes)\n",
        "    #predicted_labels = [label_encoders[0].classes_[cls] for cls in predicted_classes]\n",
        "    l.append(predicted_labels)\n",
        "    #print(np.concatenate((data[i][:j], data[i][j+1:])),end='\\n')\n",
        "  result.append(l)\n",
        "  break\n",
        "result"
      ],
      "metadata": {
        "id": "AFqN7j_BlkdF",
        "outputId": "c6f6e90f-9fb3-4c12-90f4-be7ce433ed72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "[2]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "[2]\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "[0]\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "[1]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['Very Low'], ['Very Low'], ['Very Low'], ['Very Low']]]"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "4zeDJVvEj_Va",
        "outputId": "a33edf1a-89cd-41a0-cf2a-35b2b200a789",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['Low', 'Low', 'Medium', 'Very High'],\n",
              "       ['Medium', 'Very Low', 'High', 'Low'],\n",
              "       ['High', 'Low', 'High', 'Medium'],\n",
              "       ['Very Low', 'Very Low', 'Low', 'Low']], dtype='<U9')"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.concatenate((data[0][:1], data[0][1:3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dH2rk__Kgd1A",
        "outputId": "bad5aaa4-56c2-4c52-d4bb-ad55905c9c6f"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Low', 'Low', 'Medium'], dtype='<U9')"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the new data\n",
        "new_data = np.array([\n",
        "    ['Low', 'Medium', 'Very High']\n",
        "])\n",
        "\n",
        "# Transform the new data using the fitted label encoders\n",
        "X_new_encoded = np.column_stack([le.transform(new_data[:, i]) for i, le in enumerate(label_encoders)])\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_new_encoded)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Decode predicted classes\n",
        "predicted_labels = [label_encoders[0].classes_[cls] for cls in predicted_classes]\n",
        "print(\"Predictions:\", predicted_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EKnnX3-aMv7",
        "outputId": "341e1443-8548-45e0-cd4c-3ed4f7586de6"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 58ms/step\n",
            "Predictions: ['Very Low']\n"
          ]
        }
      ]
    }
  ]
}